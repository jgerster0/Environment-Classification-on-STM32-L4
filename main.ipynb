{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init & Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import os\n",
        "import logging\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io.wavfile\n",
        "from keras import layers\n",
        "from scipy.signal import resample\n",
        "\n",
        "# set global seeds for reproducibility\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# parameters for plotting\n",
        "plt.rcParams['figure.figsize'] = (15.0, 8.0) \n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "\n",
        "\n",
        "\n",
        "# check if GPU is available\n",
        "print(\"GPU is\", \"AVAILABLE\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
        "\n",
        "\n",
        "if not os.path.exists('urbansounds'):\n",
        "    os.makedirs('urbansounds')\n",
        "\n",
        "# Load dataset\n",
        "#!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
        "#!tar -xzf urban8k.tgz -C urbansounds\n",
        "#!rm urban8k.tgz\n",
        "\n",
        "BASE_DATA_DIR = \"urbansounds/UrbanSound8K\"\n",
        "\n",
        "\n",
        "#for microphone rerecorded audio\n",
        "#BASE_DATA_DIR = \"urbansounds/recorded_urbansounds\"\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 200\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "pd_data = pd.read_csv(os.path.join(BASE_DATA_DIR, \"metadata\", \"UrbanSound8K.csv\"))\n",
        "\n",
        "\n",
        "\n",
        "# Map classID to target \n",
        "pd_data[\"target\"] = pd_data[\"classID\"]\n",
        "\n",
        "\n",
        "targets = sorted(pd_data[\"target\"].unique().tolist())\n",
        "assert len(targets) == NUM_CLASSES\n",
        "old_target_to_new_target = {old: new for new, old in enumerate(targets)}\n",
        "pd_data[\"target\"] = pd_data[\"target\"].map(lambda t: old_target_to_new_target[t])\n",
        "pd_data\n",
        "\n",
        "# Create mapping from target to category\n",
        "class_names = [None] * NUM_CLASSES  \n",
        "for old_target, new_target in old_target_to_new_target.items():\n",
        "    class_name = pd_data[pd_data[\"target\"] == new_target][\"class\"].iloc[0]\n",
        "    class_names[new_target] = class_name\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "\n",
        "def read_wav_file(path, fold, target_sr=SAMPLE_RATE):\n",
        "    full_path = os.path.join(BASE_DATA_DIR, \"audio\", f\"fold{fold}\", path)\n",
        "    wav, _ = librosa.load(full_path, sr=target_sr, res_type='kaiser_fast')\n",
        "\n",
        "    max_len = int(target_sr * 1)\n",
        "\n",
        "    \n",
        "    # Truncate if longer, pad with zeros if shorter\n",
        "    if len(wav) > max_len:\n",
        "        wav = wav[:max_len]\n",
        "    elif len(wav) < max_len:\n",
        "        wav = np.pad(wav, (0, max_len - len(wav)))\n",
        "\n",
        "    return wav\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_dataset(df, folds):\n",
        "    msk = df[\"fold\"].isin(folds)\n",
        "    filenames = df[\"slice_file_name\"][msk].values\n",
        "    foldnums  = df[\"fold\"][msk].values\n",
        "    targets   = df[\"target\"][msk].values\n",
        "    waves = np.array([read_wav_file(f, fold) for f, fold in zip(filenames, foldnums)], dtype=np.object_)\n",
        "    return waves, targets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract data\n",
        "filenames = pd_data[\"slice_file_name\"].values\n",
        "foldnums  = pd_data[\"fold\"].values\n",
        "targets   = pd_data[\"target\"].values \n",
        "\n",
        "print(\"Loading all audio data\")\n",
        "waves = np.array([read_wav_file(fname, fold) for fname, fold in zip(filenames, foldnums)], dtype=np.object_)\n",
        "targets = np.array(targets)\n",
        "\n",
        "\n",
        "\n",
        "# 70% train, 30% temp (to be split into val/test)\n",
        "train_x, temp_x, train_y, temp_y = train_test_split(\n",
        "    waves, targets,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=targets\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rerecord_data import *\n",
        "#rerecord data with microphone\n",
        "\n",
        "#record_entire_dataset(pd_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 15% val, 15% test\n",
        "valid_x, test_x, valid_y, test_y = train_test_split(\n",
        "    temp_x, temp_y,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=temp_y\n",
        ")\n",
        "\n",
        "print(f\"Number of segments for: train_x: {len(train_x)}, valid_x: {len(valid_x)}, test_x: {len(test_x)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "7w7gDy6bCM07",
        "outputId": "4bcb4a98-d40a-4e86-8202-1d619f971553"
      },
      "outputs": [],
      "source": [
        "\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain, HighPassFilter, LowPassFilter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "from audiomentations import (\n",
        "    Compose, AddColorNoise, Shift, Gain,\n",
        "    HighPassFilter, LowPassFilter, PolarityInversion, \n",
        "    ClippingDistortion,  PitchShift, AddBackgroundNoise\n",
        ")\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "                \n",
        "\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    AddColorNoise(\n",
        "        min_snr_db=15, max_snr_db=18, p=0.5,  # “pink” by default\n",
        "    ),\n",
        "    Gain(min_gain_db=-6.0, max_gain_db=6.0, p=0.5)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "esc50_path = \"\"\n",
        "MAX_SHIFT_S = 0.15   \n",
        "augment = Compose([\n",
        "\n",
        "    AddBackgroundNoise(sounds_path=esc50_path, min_snr_in_db=5.0, max_snr_in_db=20.0, p=1.0),\n",
        "\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "\n",
        "    AddColorNoise(min_snr_db=15, max_snr_db=18, p=0.5,),  # pink by default\n",
        "\n",
        "    Gain(min_gain_db=-6.0, max_gain_db=6.0, p=0.5),\n",
        "\n",
        "    LowPassFilter( min_cutoff_freq=3000., max_cutoff_freq=6000., p=0.25),\n",
        "\n",
        "])\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def apply_augmentations(x, y, sample_rate=16000):\n",
        "    aug = [augment(samples=wav[None, :].astype(np.float32),\n",
        "                   sample_rate=sample_rate)[0] for wav in x]\n",
        "    x_aug = np.stack(aug, axis=0)              \n",
        "    y_aug = y.copy()                           # duplicate labels\n",
        "\n",
        "    x_ext = np.concatenate([x, x_aug], axis=0)\n",
        "    y_ext = np.concatenate([y, y_aug], axis=0)\n",
        "\n",
        "    assert x_ext.shape[0] == y_ext.shape[0]\n",
        "    return x_ext, y_ext\n",
        "\n",
        "\n",
        "\n",
        "train_x_augmented, train_y_augmented = apply_augmentations(train_x, train_y)\n",
        "\n",
        "\n",
        "\n",
        "print(train_x_augmented.shape)\n",
        "print(train_y_augmented.shape)\n",
        "\n",
        "\n",
        "\n",
        "train_x = train_x_augmented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create MFCCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import cmsisdsp as dsp\n",
        "import cmsisdsp.mfcc as mfcc\n",
        "import scipy.signal.windows as sig\n",
        "from cmsisdsp.datatype import F32\n",
        "\n",
        "\n",
        "# parameters \n",
        "FFTSize         = 1024\n",
        "numOfMelFilters = 80\n",
        "numOfDctOutputs = 16\n",
        "freq_min        = 80.0\n",
        "freq_high       = 7600.0\n",
        "sample_rate     = 16000.0\n",
        "frame_length    = 1024 \n",
        "frame_step      = 512\n",
        "\n",
        "\n",
        "# Create the Hamming window\n",
        "window = sig.hamming(FFTSize, sym=False)\n",
        "# Create mel filters\n",
        "filtLen, filtPos, packedFilters = mfcc.melFilterMatrix(\n",
        "    F32, freq_min, freq_high,\n",
        "    numOfMelFilters, sample_rate, FFTSize\n",
        ")\n",
        "\n",
        "# Create DCT matrix\n",
        "dctMatrixFilters = mfcc.dctMatrix(F32, numOfDctOutputs, numOfMelFilters)\n",
        "\n",
        "# initialize arm_mfcc_instance_f32\n",
        "mfccf32 = dsp.arm_mfcc_instance_f32()\n",
        "status = dsp.arm_mfcc_init_f32(\n",
        "    mfccf32,\n",
        "    FFTSize,\n",
        "    numOfMelFilters,\n",
        "    numOfDctOutputs,\n",
        "    dctMatrixFilters,\n",
        "    filtPos,\n",
        "    filtLen,\n",
        "    packedFilters,\n",
        "    window\n",
        ")\n",
        "if status != 0:\n",
        "    raise RuntimeError(\"MFCC init failed (status code = {})\".format(status))\n",
        "\n",
        "\n",
        "\n",
        "def compute_mfccs_cmsis_batch(x_data, mfcc_instance,\n",
        "                              numDctOutputs,  \n",
        "                              frame_length=1024, frame_step=512):\n",
        "    \"\"\"\n",
        "    x_data:         NumPy array of shape [N, num_samples], float32\n",
        "    mfcc_instance:  The initialized arm_mfcc_instance_f32\n",
        "    numDctOutputs:  The number of MFCC coefficients \n",
        "    frame_length:   Number of samples per frame\n",
        "    frame_step:     Hop size\n",
        "    Returns:        MFCC array of shape [N, num_frames, numDctOutputs].\n",
        "    \"\"\"\n",
        "    N, num_samples = x_data.shape\n",
        "\n",
        "    # frames per sample\n",
        "    num_frames = (num_samples - frame_length) // frame_step + 1\n",
        "\n",
        "    # storing mfccs in array: (N, num_frames, numDctOutputs)\n",
        "    all_mfccs = np.zeros((N, num_frames, numDctOutputs), dtype=np.float32)\n",
        "\n",
        "    # Temporary buffer for arm_mfcc_f32:\n",
        "    tmp = np.zeros(frame_length + 2, dtype=np.float32)\n",
        "\n",
        "    for i in range(N):\n",
        "        signal = x_data[i]  # shape: (16000,)\n",
        "\n",
        "        for f in range(num_frames):\n",
        "            start = f * frame_step\n",
        "            end   = start + frame_length\n",
        "\n",
        "            frame_data = signal[start:end].astype(np.float32)\n",
        "\n",
        "            # 1-frame MFCC via CMSIS\n",
        "            mfcc_result = dsp.arm_mfcc_f32(mfcc_instance, frame_data, tmp)\n",
        "\n",
        "            # Store result\n",
        "            all_mfccs[i, f, :] = mfcc_result\n",
        "\n",
        "    return all_mfccs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_cmsis_mfccs = compute_mfccs_cmsis_batch(\n",
        "    train_x,\n",
        "    mfccf32,\n",
        "    numDctOutputs=numOfDctOutputs,  \n",
        "    frame_length=1024,\n",
        "    frame_step=512\n",
        ")\n",
        "\n",
        "\n",
        "valid_cmsis_mfccs = compute_mfccs_cmsis_batch(\n",
        "    valid_x,\n",
        "    mfccf32,\n",
        "    numDctOutputs=numOfDctOutputs,  \n",
        "    frame_length=1024,\n",
        "    frame_step=512\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_cmsis_mfccs = compute_mfccs_cmsis_batch(\n",
        "    test_x,\n",
        "    mfccf32,\n",
        "    numDctOutputs=numOfDctOutputs,  \n",
        "    frame_length=1024,\n",
        "    frame_step=512\n",
        ")\n",
        "\n",
        "\n",
        "print(train_cmsis_mfccs.shape)\n",
        "print(valid_cmsis_mfccs.shape)\n",
        "print(test_cmsis_mfccs.shape)\n",
        "\n",
        "\n",
        "\n",
        "train_x_mfcc = tf.reshape(train_cmsis_mfccs, (train_cmsis_mfccs.shape[0], train_cmsis_mfccs.shape[1], train_cmsis_mfccs.shape[2], -1))\n",
        "print(train_x_mfcc.shape)\n",
        "print(train_y_augmented.shape)\n",
        "\n",
        "\n",
        "valid_x_mfcc = tf.reshape(valid_cmsis_mfccs, (valid_cmsis_mfccs.shape[0], valid_cmsis_mfccs.shape[1], valid_cmsis_mfccs.shape[2], -1))\n",
        "print(valid_x_mfcc.shape)\n",
        "print(valid_y.shape)\n",
        "\n",
        "test_x_mfcc = tf.reshape(test_cmsis_mfccs, (test_cmsis_mfccs.shape[0], test_cmsis_mfccs.shape[1], test_cmsis_mfccs.shape[2], -1))\n",
        "print(test_x_mfcc.shape)\n",
        "print(f\"test_y Shape: {test_y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "gWzj7mUSCM0-",
        "outputId": "2dc491f1-d4e9-474b-8780-9a326c61fdbf"
      },
      "outputs": [],
      "source": [
        "#Compute Compression Ratio\n",
        "\n",
        "total_train_size_no_compression = train_x.nbytes\n",
        "total_train_size_with_compression = train_x_mfcc.numpy().nbytes\n",
        "\n",
        "print(\"Total training data size without compression: \", total_train_size_no_compression)\n",
        "print(\"Total training data size with compression: \", total_train_size_with_compression)\n",
        "print(\"Compression ratio: \", total_train_size_no_compression/total_train_size_with_compression)\n",
        "print(1- total_train_size_with_compression/total_train_size_no_compression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print mfcc frames (for Debugging)\n",
        "x_mfcc_np = test_x_mfcc.numpy()  \n",
        "\n",
        "\n",
        "\n",
        "for frame in range(x_mfcc_np.shape[1]): \n",
        "    print(f\"Frame {frame:02d}:\", end=\" \")\n",
        "    for coeff in x_mfcc_np[0, frame, :, 0]:  \n",
        "        print(f\"{coeff:.6f}\", end=\" \")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzHlsmfPCM0-"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#training with keras tuner for hyperparameter tuning\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    l2_val = hp.Float(\"l2_reg\", 1e-5, 1e-2, sampling=\"log\")\n",
        "    dropout_1 = hp.Float(\"dropout_1\", 0.1, 0.5, step=0.05)\n",
        "    dropout_2 = hp.Float(\"dropout_2\", 0.1, 0.5, step=0.05)\n",
        "    act_fn = hp.Choice(\"activation\", [\"relu\", \"linear\"])\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(30, 16, 1)),\n",
        "\n",
        "        layers.Conv2D(16, kernel_size=(2, 4), activation=act_fn, padding=\"same\",\n",
        "                      kernel_regularizer=regularizers.l2(l2_val)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 3)),\n",
        "\n",
        "        layers.Conv2D(32, kernel_size=(2, 4), activation=act_fn, padding=\"same\",\n",
        "                      kernel_regularizer=regularizers.l2(l2_val)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size=(2, 4), activation=act_fn, padding=\"same\",\n",
        "                      kernel_regularizer=regularizers.l2(l2_val)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(dropout_1),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size=(2, 4), activation=\"relu\", padding=\"same\",\n",
        "                      kernel_regularizer=regularizers.l2(l2_val)),\n",
        "\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(dropout_2),\n",
        "\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='urban8k_tuning'\n",
        ")\n",
        "\n",
        "tuner.search(train_x_mfcc, train_y_augmented, epochs=10, validation_data=(test_x_mfcc, test_y), batch_size=256)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import visualkeras\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "baseline_model = tuner.get_best_models(num_models=1)[0]\n",
        "baseline_model.summary()\n",
        "\n",
        "\n",
        "# visualize model\n",
        "def show_model_visualization(model, title):\n",
        "    img = visualkeras.layered_view(model, legend=True)  \n",
        "    img_path = f\"{title}.png\"\n",
        "    img.save(img_path)  \n",
        "    display.display(Image.open(img_path))  \n",
        "\n",
        "\n",
        "print(\"Model Architecture:\\r\\n\")\n",
        "show_model_visualization(baseline_model, \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Convert to tflite\n",
        "\n",
        "model_name = \"baseline\"\n",
        "\n",
        "baseline_model_loss, baseline_model_acc = baseline_model.evaluate(test_x_mfcc, test_y)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n",
        "baseline_tflite_model = converter.convert()\n",
        "\n",
        "# Save tflite model \n",
        "open(f\"models/{model_name}.tflite\", \"wb\").write(baseline_tflite_model)\n",
        "\n",
        "\n",
        "\n",
        "# Show model size \n",
        "baseline_size = os.path.getsize(f'models/{model_name}.tflite') / 1024\n",
        "print(\"Baseline TFLite Model size : %d KB\" % baseline_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Quantization Aware Training (QAT) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.quantization.keras import QuantizeConfig\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "# Custom NoOpQuantizeConfig (Skips BatchNormalization Quantization)\n",
        "class NoOpQuantizeConfig(QuantizeConfig):\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "        return []\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "        return []\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "        pass\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "        pass\n",
        "    def get_output_quantizers(self, layer):\n",
        "        return []\n",
        "    def get_config(self):\n",
        "        return {}\n",
        "\n",
        "# Annotate Model with Custom Config for BatchNormalization\n",
        "def apply_quantization_with_custom_config(model):\n",
        "    def quantize_layer(layer):\n",
        "        if isinstance(layer, BatchNormalization):\n",
        "            return tfmot.quantization.keras.quantize_annotate_layer(layer, quantize_config=NoOpQuantizeConfig())\n",
        "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "\n",
        "    return clone_model(model, clone_function=quantize_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "quant_aware_annotate_model = apply_quantization_with_custom_config(baseline_model)\n",
        "\n",
        "# quantize_scope to register custom objects\n",
        "with tfmot.quantization.keras.quantize_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig}):\n",
        "    qat_model = tfmot.quantization.keras.quantize_apply(\n",
        "        quant_aware_annotate_model,\n",
        "        tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme()\n",
        "    )\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003, clipnorm=1.0)\n",
        "\n",
        "qat_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "qat_model.summary()\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 50:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.03) \n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=100, restore_best_weights=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
        "    tfmot.sparsity.keras.UpdatePruningStep()\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "EPOCHS = 250\n",
        "history = qat_model.fit(\n",
        "    train_x_mfcc,\n",
        "    train_y_augmented,\n",
        "    validation_data=(valid_x_mfcc, valid_y),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=256,\n",
        "    callbacks=callbacks,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate the model on the test set\n",
        "qat_loss, qat_acc = qat_model.evaluate(test_x_mfcc, test_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QAT to TFLite and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#helper functions\n",
        "\n",
        "def evaluate_tflite_model(tflite_model_path, x_test, y_test):\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "\n",
        "    correct = 0\n",
        "    total = x_test.shape[0]\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(total):\n",
        "        float_img = x_test[i:i+1]\n",
        "        int8_img = np.round(float_img / input_scale + input_zero_point).astype(np.int8)\n",
        "        interpreter.set_tensor(input_details[0]['index'], int8_img)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        pred_label = np.argmax(output_data[0])\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "        if pred_label == y_test[i]:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, np.array(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tempfile\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "    # returns size of the gzipped model in bytes.\n",
        "    import os\n",
        "    import zipfile\n",
        "\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "        f.write(file)\n",
        "\n",
        "    return os.path.getsize(zipped_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plot the confusion matrix \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "\"\"\"\n",
        "function: plot_confusion_matrix\n",
        "    - input: cm, classes, normalize, title, cmap\n",
        "    - output: none\n",
        "    - description: plots the confusion matrix\n",
        "\"\"\"\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                            normalize=True,\n",
        "                            title='Confusion matrix',\n",
        "                            cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#convert to tflite\n",
        "\n",
        "def representative_data_gen():\n",
        "    # Provide a small subset of training data for calibration\n",
        "    for i in range(100):\n",
        "        yield [train_x_mfcc[i:i+1]]\n",
        "\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "qat_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Save the model\n",
        "qat_tflite_file = f'models/final_qat.tflite'\n",
        "\n",
        "with open(qat_tflite_file, 'wb') as f:\n",
        "    f.write(qat_tflite_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate qat tflite model on the test set\n",
        "\n",
        "quantized_acc, y_pred = evaluate_tflite_model(qat_tflite_file, test_x_mfcc, test_y)\n",
        "print(f\"Quantized TFLite model test accuracy: {quantized_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate qat tflite model on test set\n",
        "\n",
        "quantized_acc, y_pred = evaluate_tflite_model(qat_tflite_file, test_x_mfcc, test_y)\n",
        "print(f\"Quantized TFLite model test accuracy: {quantized_acc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(test_y, y_pred)\n",
        "plot_confusion_matrix(cm, class_names, title='Normalized Confusion matrix of the 8bit QAT model', cmap=plt.cm.Reds)\n",
        "\n",
        "\n",
        "\n",
        "qat_size = os.path.getsize(qat_tflite_file) / 1024\n",
        "print(\"QAT Model size: %d KB\" % qat_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert to h file (for tflite micro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function: Convert some hex value into an array for C programming\n",
        "def hex_to_c_array(hex_data, var_name):\n",
        "\n",
        "    c_str = ''\n",
        "\n",
        "    # Create header guard\n",
        "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
        "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
        "\n",
        "    # Add array length at top of file\n",
        "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
        "\n",
        "    # Declare C variable\n",
        "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
        "    hex_array = []\n",
        "    for i, val in enumerate(hex_data) :\n",
        "\n",
        "        # Construct string from hex\n",
        "        hex_str = format(val, '#04x')\n",
        "\n",
        "        # Add formatting so each line stays within 80 characters\n",
        "        if (i + 1) < len(hex_data):\n",
        "            hex_str += ','\n",
        "        if (i + 1) % 12 == 0:\n",
        "            hex_str += '\\n '\n",
        "        hex_array.append(hex_str)\n",
        "\n",
        "    # Add closing brace\n",
        "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
        "\n",
        "    # Close out header guard\n",
        "    c_str += '#endif //' + var_name.upper() + '_H'\n",
        "\n",
        "    return c_str\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "# Make directory for output\n",
        "if not os.path.exists('cfiles'):\n",
        "    os.makedirs('cfiles')\n",
        "\n",
        "# Path to model\n",
        "model_path = qat_tflite_file\n",
        "c_model_name = \"final_qat\"\n",
        "\n",
        "# Load TFLite model as bytes\n",
        "with open(model_path, 'rb') as f:\n",
        "    tflite_model_content = f.read()\n",
        "\n",
        "# Convert and save as C header\n",
        "with open(f'cfiles/{c_model_name}.h', 'w') as f:\n",
        "    f.write(hex_to_c_array(tflite_model_content, c_model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "#helper functions\n",
        "\n",
        "def plot_wav(wav):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    librosa.display.waveshow(wav, sr=16000)\n",
        "    plt.title(\"Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def test_read_wav_file(path, target_sr=SAMPLE_RATE):\n",
        "    full_path = path\n",
        "    wav, _ = librosa.load(full_path, sr=target_sr, res_type='kaiser_fast')\n",
        "\n",
        "    max_len = int(target_sr * 1)\n",
        "\n",
        "    # Truncate if longer, pad with zeros if shorter\n",
        "    if len(wav) > max_len:\n",
        "        wav = wav[:max_len]\n",
        "    elif len(wav) < max_len:\n",
        "        wav = np.pad(wav, (0, max_len - len(wav)))\n",
        "\n",
        "    return wav\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "import time\n",
        "\n",
        "\n",
        "def play_and_record(\n",
        "    wav_path: str,\n",
        "    output_path: str | None = None,\n",
        "    playback_device=None,\n",
        "    record_device=None,\n",
        "    keep_copy: bool = False,\n",
        "):\n",
        "    \"\"\"Play WAV and record  int16 16 kHz audio to WAV.\"\"\"\n",
        "    src, file_sr = sf.read(wav_path, dtype=\"float32\")\n",
        "    if file_sr != 16000:\n",
        "        src = librosa.resample(src.T, orig_sr=file_sr, target_sr=16000).T\n",
        "    out_ch = src.shape[1] if src.ndim == 2 else 1\n",
        "\n",
        "    if output_path is None:\n",
        "        stem, ext = os.path.splitext(wav_path)\n",
        "        output_path = f\"{stem}_recorded{ext}\"\n",
        "\n",
        "\n",
        "    print(f\"Playing {os.path.basename(output_path)}\")\n",
        "\n",
        "    # Record\n",
        "    recording = sd.playrec(\n",
        "        src,\n",
        "        samplerate=16000,\n",
        "        channels=1,\n",
        "        dtype=\"int16\",           \n",
        "        blocksize=1024,\n",
        "        device=(playback_device, record_device) if (playback_device or record_device) else None,\n",
        "    )\n",
        "    sd.wait()\n",
        "\n",
        "    # Save as int16 PCM\n",
        "    sf.write(output_path, recording, 16000, subtype=\"PCM_16\")\n",
        "\n",
        "\n",
        "    return (output_path, recording) if keep_copy else output_path\n",
        "\n",
        "\n",
        "def safe_wav(wav):\n",
        "    fold = os.path.basename(os.path.dirname(wav))\n",
        "    safe = os.path.join('test_wavs', 'recorded_'+ fold+ '_' + os.path.basename(wav))\n",
        "    return safe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def record_microphone(\n",
        "    duration: float,\n",
        "    output_path: str,\n",
        "    samplerate: int = 16000,\n",
        "    record_device=None,\n",
        "    channels: int = 1,\n",
        "    dtype: str = \"int16\",\n",
        "    blocksize: int = 1024,\n",
        "    save_npy: bool = False, \n",
        "    sound_name: str = None   \n",
        "):\n",
        "    print(f\"Recording for {duration} seconds\")\n",
        "\n",
        "    recording = sd.rec(\n",
        "        int(duration * samplerate),\n",
        "        samplerate=samplerate,\n",
        "        channels=channels,\n",
        "        dtype=dtype,\n",
        "        blocksize=blocksize,\n",
        "        device=record_device\n",
        "    )\n",
        "    sd.wait()\n",
        "\n",
        "    print(f\"Saving WAV to {output_path}\")\n",
        "    sf.write(output_path, recording, samplerate, subtype=\"PCM_16\")\n",
        "\n",
        "  \n",
        "    if save_npy and sound_name:\n",
        "        npy_path = f\"test_wavs/rec_{sound_name}.npy\"\n",
        "        print(f\"Saving NPY to {npy_path}\")\n",
        "        np.save(npy_path, recording)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "\n",
        "#record sound with microphone\n",
        "\n",
        "#sound_name = 'dog'\n",
        "#time.sleep(1)\n",
        "#safe = os.path.join('test_wavs', f'rec_{sound_name}.wav')\n",
        "#captured = record_microphone(2, safe, save_npy=True, sound_name=sound_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPI3wNm5xavd",
        "outputId": "50c86a85-a638-479c-e6e0-3a89ac7af96b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#test audio from recordings\n",
        "if(1):\n",
        "    sound = 'siren'\n",
        "\n",
        "    #wav audio \n",
        "\n",
        "    path = f\"test_wavs/rec_{sound}.wav\"\n",
        "    processed_record = test_read_wav_file(path, SAMPLE_RATE)\n",
        "\n",
        "\n",
        "    #npy audio \n",
        "    #processed_record = np.load(\"test_wavs/rec_{sound}.npy\")\n",
        "    #processed_record = (processed_record.astype(np.float32).reshape(1, -1))/ 32768.0\n",
        "\n",
        "\n",
        "    #plot_wav(processed_record)\n",
        "\n",
        "\n",
        "processed_record = (processed_record.astype(np.float32).reshape(1, -1))\n",
        "\n",
        "\n",
        "\n",
        "print(processed_record.shape)\n",
        "mfccs = compute_mfccs_cmsis_batch(\n",
        "    processed_record,\n",
        "    mfccf32,\n",
        "    numDctOutputs=numOfDctOutputs,  \n",
        "    frame_length=1024,\n",
        "    frame_step=512\n",
        ")\n",
        "x_mfcc_tf = tf.reshape(mfccs, (mfccs.shape[0], mfccs.shape[1], mfccs.shape[2], -1))\n",
        "print(x_mfcc_tf.shape)\n",
        "\n",
        "\n",
        "#predictions = baseline_model.predict(x_mfcc_tf)\n",
        "predictions = qat_model.predict(x_mfcc_tf)\n",
        "\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "predicted_labels = [class_names[i] for i in predicted_class]\n",
        "\n",
        "print(f\"Predicted Class: {predicted_labels}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking with other Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# my models\n",
        "model_types = ['Full Precision', 'QAT']\n",
        "model_accuracies = [100 * baseline_model_acc, 100 * quantized_acc]\n",
        "model_accuracies = [round(x, 4) for x in model_accuracies]\n",
        "model_sizes = [baseline_size, qat_size]\n",
        "model_sizes = [round(x) for x in model_sizes]  # KB\n",
        "\n",
        "# models from papers\n",
        "external_models = [\n",
        "    \"DenseNet-201 [1]\",\n",
        "    \"YAMNet [2]\",\n",
        "    \"AST [3]\",\n",
        "    \"NeuProNet [4]\",\n",
        "    \"Distilled Transformer [5]\",\n",
        "    \"ESC-NAS [6]\",\n",
        "    \"Micro-ACDNet [7]\"\n",
        "]\n",
        "external_accuracies = [97.25, 95.3, 89.8, 83.75, 83.3, 81.25, 79.0]\n",
        "external_sizes = [80 * 1024, 15 * 1024, 344 * 1024, 21 * 1024, 3 * 1024, 359, 300]  # KB\n",
        "\n",
        "# Combine\n",
        "all_models = model_types + external_models\n",
        "all_accuracies = model_accuracies + external_accuracies\n",
        "all_sizes = model_sizes + [round(s) for s in external_sizes]\n",
        "\n",
        "# Pretty table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Model type\", \"Accuracy (%)\", \"Size\"]\n",
        "for i in range(len(all_models)):\n",
        "    size_str = f\"{round(all_sizes[i]/1024, 2)} MB\" if all_sizes[i] >= 1024 else f\"{all_sizes[i]} KB\"\n",
        "    table.add_row([all_models[i], all_accuracies[i], size_str])\n",
        "print(table)\n",
        "\n",
        "# Log and dot scaling\n",
        "log_sizes = [np.log10(s) for s in all_sizes]\n",
        "dot_sizes = [100 + 200 * ls for ls in log_sizes]  \n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 8))\n",
        "plot = sns.scatterplot(x=all_sizes, y=all_accuracies, size=dot_sizes, sizes=(100, 1000), hue=all_models, legend=False)\n",
        "plt.xscale('log')  # Log axis\n",
        "plt.title('Model Accuracy vs Size: Benchmark Against Related Models')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Model Size (KB)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate \n",
        "for i in range(len(all_models)):\n",
        "    size_label = f\"{round(all_sizes[i]/1024, 1)} MB\" if all_sizes[i] >= 1024 else f\"{all_sizes[i]} KB\"\n",
        "    plt.annotate(all_models[i], (all_sizes[i], all_accuracies[i] + 0.5), fontsize=15, ha='center')\n",
        "    plt.annotate(size_label, (all_sizes[i], all_accuracies[i] - 0.7), fontsize=14, ha='center', color='gray')\n",
        "\n",
        "\n",
        "for line in plot.texts:\n",
        "    if line.get_text() in [\"Full Precision\", \"QAT\"]:\n",
        "        line.set_color(\"red\")\n",
        "\n",
        "# references\n",
        "source_text = (\n",
        "    \"[1] DenseNet-201: Huang et al., CVPR 2017\\n\"\n",
        "    \"[2] YAMNet: Google AI Blog, 2020\\n\"\n",
        "    \"[3] AST: Gong et al., ICASSP 2021\\n\"\n",
        "    \"[4] NeuProNet: Wang et al., Neurocomputing 2022\\n\"\n",
        "    \"[5] Distilled Transformer: Gong et al., ICASSP 2022\\n\"\n",
        "    \"[6] ESC-NAS: Nagrani et al., 2022\\n\"\n",
        "    \"[7] Micro-ACDNet: Yu et al., 2022\"\n",
        ")\n",
        "plt.figtext(0.99, -0.15, source_text, wrap=True, horizontalalignment='right', fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
